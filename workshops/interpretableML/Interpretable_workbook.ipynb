{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1FolxS9J2PRECgbgpXeRvqQt-dbqHoXus","timestamp":1706568202270},{"file_id":"14MdPtdaHAQV44GAXtiG4bK0Ov64Nncl-","timestamp":1706550980903}],"collapsed_sections":["qCpRkbd8XjyO","wwg7gxEyJs6r","CAErVLMXrdPB","3BMhYt_N7qnu","OUbqwNSjJyE2","YrQnmldUL-sf","Cf8NZ2fw4zIR","TBmyJRCJ4cLz","pL0qEXGYW-vb","GTh5AuhHCe5p","Q-revrz6Cgv4","tw82OIFstDka"],"authorship_tag":"ABX9TyOcaMGtw6l6vTXWBEuvogP/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Interpretable machine learning\n","\n","In this week's workshop you will explore various interpretable machine learning methods. I have marked out lines where you need to fill in with #** EDIT ME **"],"metadata":{"id":"qCpRkbd8XjyO"}},{"cell_type":"code","source":["!pip install lime\n","!pip install datasets\n","!pip install shap"],"metadata":{"id":"-n8aC_ZVeaoy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","import tensorflow.keras.layers as tkl\n","import os\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import tensorflow_datasets as tfds\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","\n","#for LIME\n","import requests\n","import lime\n","from lime import lime_text\n","from lime.lime_text import LimeTextExplainer\n","from lime import lime_image\n","from skimage.segmentation import mark_boundaries\n","from datasets import load_dataset\n","import skimage.segmentation\n","from sklearn.linear_model import Ridge\n","\n","#for SHAP\n","import shap"],"metadata":{"id":"JgnE0aEYcv1w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# print the JS visualization code to the notebook\n","shap.initjs()"],"metadata":{"id":"oM-V0LFo_BYD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Partial dependence plots\n","\n","Let's first look at PDPs. PDPs offer a visual way to analyze the individual effects of features on model predictions. This allows us to deconstruct the complex relationships within a model and gain insights into its decision-making process.\n","\n","## Load Boston housing price regresssion dataset\n","\n","\n","The Boston housing dataset is a well-established benchmark for evaluating and interpreting machine learning models. It consists of 13 attributes of houses at different locations around the Boston suburbs in the late 1970s. These include  the number of rooms, crime rate, and distance to the highway, along with the target variable of median house prices at a location (in k$). See https://lib.stat.cmu.edu/datasets/boston for column names.   \n","\n"],"metadata":{"id":"wwg7gxEyJs6r"}},{"cell_type":"code","source":["(x_train, y_train), (x_test, y_test) = tf.keras.datasets.boston_housing.load_data()"],"metadata":{"id":"0SCt9m_QIELG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# start by normalising the data to standard normal\n","x_train =  #** EDIT ME **\n","x_test =  #** EDIT ME **\n","y_train =  #** EDIT ME **\n","y_test =  #** EDIT ME **"],"metadata":{"id":"7Jrw859EpxpI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tf.keras.backend.clear_session()"],"metadata":{"id":"MFYKSsqPS5Ss"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# define a feed forward model to predict the price\n","model = tf.keras.Sequential()\n","model.add(tkl.Dense(13, input_dim=13, activation='relu'))\n","model.add(tkl.Dense(13, activation='relu'))\n","model.add(tkl.Dense(5, activation='relu'))\n","model.add(tkl.Dense(1, activation=None))"],"metadata":{"id":"pN2685YTIUxy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.summary()"],"metadata":{"id":"xUv1fufITPNR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["opt = tf.keras.optimizers.Adam(learning_rate=0.001) #define optimiser\n","model.compile(loss='mean_squared_error', optimizer=opt ,metrics=['MAE']) #compile model"],"metadata":{"id":"JTuERQrITNxn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# train your model\n","history = model.fit(x_train, y_train, epochs=100, validation_data=(x_test, y_test), batch_size=32)"],"metadata":{"id":"T8Y0tyfiSSj9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# This function creates a PDP plot for 1 feature\n","def make_pdp(data, feature_index, steps=50):\n","  if np.size(feature_index)==1:\n","    # Generate a range of values for the feature\n","    start = int(np.min(data[:,feature_index]))\n","    end = int(np.max(data[:,feature_index]))\n","    feature_values = np.linspace(start, end, steps)\n","\n","    # Store the average predictions\n","    average_predictions = []\n","\n","    for value in feature_values:\n","        # Modify the data to set the feature of interest to the current value\n","        modified_data = np.copy(data)\n","        modified_data[:, feature_index] = value\n","\n","        # Make predictions\n","        predictions = model.predict(modified_data, verbose=0 )\n","\n","        # Calculate and store the average prediction\n","        average_predictions.append(np.mean(predictions))\n","\n","    # Plotting the PDP\n","    plt.plot(feature_values, average_predictions)\n","    plt.xlabel('Feature Value')\n","    plt.ylabel('Average Prediction')\n","    plt.title('Partial Dependence Plot')\n","    plt.show()"],"metadata":{"id":"qRSnH4rM2188"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# plot the PDP for feature number of rooms\n","make_pdp(x_train, feature_index=5, steps=10)"],"metadata":{"id":"YEmchQLF4zy3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["What do you see?  #** EDIT ME **"],"metadata":{"id":"G554dKw0gJtz"}},{"cell_type":"code","source":["# YOUR TURN, fill in this function to make a PDP plot for 2 features\n","def make_pdp(data, feature_index, steps=50):\n","    # Generate ranges of values for the two features\n","    feature_values_1 =  #** EDIT ME **\n","    feature_values_2 =  #** EDIT ME **\n","\n","\n","    # Initialize a grid to store the average predictions\n","    average_predictions = np.zeros((len(feature_values_1), len(feature_values_2)))\n","\n","    # Iterate over all combinations of feature values\n","    for i, value_1 in enumerate(feature_values_1):\n","        for j, value_2 in enumerate(feature_values_2):\n","            # Modify the data for the current combination of feature values\n","            modified_data =  #** EDIT ME **\n","\n","            # Make predictions and store the average\n","            predictions =  #** EDIT ME **\n","            average_predictions[i, j] =  #** EDIT ME **\n","\n","    # Plotting the two-way PDP as a heatmap\n","    plt.imshow(average_predictions, interpolation='nearest')\n","    plt.xlabel('Feature 1 Value')\n","    plt.ylabel('Feature 2 Value')\n","    plt.colorbar(label='Average Prediction')\n","    plt.title('Two-Way Partial Dependence Plot')\n","    plt.show()"],"metadata":{"id":"Gbb9dbm5ed4N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# plot the PDP for feature crime rate and number of bedrooms, note this takse a while to run\n","make_pdp(x_train,\n","         feature_index= #** EDIT ME **\n","         , steps=20)"],"metadata":{"id":"k3D6ThGFfPXG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["What do you see?\n"," #** EDIT ME **\n",""],"metadata":{"id":"7ASaOygmo59w"}},{"cell_type":"markdown","source":["# ICE plots\n","Now lets moove on to ICE plots which show individual contributions rather than the average which can help us identify outliers"],"metadata":{"id":"CAErVLMXrdPB"}},{"cell_type":"code","source":["def make_ice(data, feature_index, steps=20):\n","    # Generate a range of values for the feature\n","    start = int(np.min(data[:,feature_index]))\n","    end = int(np.max(data[:,feature_index]))\n","    feature_values = np.linspace(start, end, steps)\n","\n","    predictions = np.zeros((steps, len(data)))\n","\n","    for i, value in enumerate(feature_values):\n","        # Modify the data to set the feature of interest to the current value\n","        modified_data = np.copy(data)\n","        modified_data[:, feature_index] = value\n","\n","        # Make predictions\n","        predictions[i,:] = model.predict(modified_data, verbose=0 )[:,0]\n","\n","    for j in range(len(data)):\n","        anchor = predictions[0,j]\n","        # Plotting the ICE centred to 0 at first feature\n","        plt.plot(feature_values, predictions[:,j]-anchor, c='black', alpha=0.2)\n","    # plot average (PDP)\n","    average_prediction = np.mean(predictions, axis=1)\n","    anchor = average_prediction[0]\n","    plt.plot(feature_values, average_prediction-anchor, c='orange', linestyle='--')\n","    plt.xlabel('Feature Value')\n","    plt.ylabel('Partial dependence')\n","    plt.title('ICE Plot')\n","    plt.show()"],"metadata":{"id":"umO9GzuhYKIz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["make_ice(x_train, feature_index=5)"],"metadata":{"id":"v-JANFXPc2xF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Here we can see above the mean feature value (i.e. 6 bedrooms) is typically when house prices increase but there are many exceptions. No obvious outliers here."],"metadata":{"id":"F_U4REAmbkL1"}},{"cell_type":"markdown","source":["# Using LIME with text data\n","\n","\n","PDP and ICE plots work well for tasks where predictions are continuous and features contributions are additive. For tasks with classification, whilst possible its better to look at other model agnostic methods like LIME. Here we apply it for Sentiment analysis to determine whether a text phrase is positive or negative."],"metadata":{"id":"3BMhYt_N7qnu"}},{"cell_type":"code","source":["# Load the Sentiment140 dataset from Huggingface (tfds link down)\n","dataset = load_dataset(\"sentiment140\")\n","\n","# Accessing the dataset\n","train_dataset = dataset['train']\n","test_dataset = dataset['test'] #we just use test set because smaller so quicker"],"metadata":{"id":"r3DDtAIT6xp4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Preprocess text data (tokenization, padding)\n","tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=10000, oov_token='<OOV>')\n","tokenizer.fit_on_texts(dataset['test']['text'])\n","\n","# Tokenize and pad sequences\n","def get_sequences(tokenizer, texts):\n","    sequences = tokenizer.texts_to_sequences(texts)\n","    padded = tf.keras.preprocessing.sequence.pad_sequences(sequences, padding='post', maxlen=120)\n","    return padded\n","\n","x_seq = get_sequences(tokenizer, dataset['test']['text'])"],"metadata":{"id":"PxmpyavT8hZv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y = np.array([0 if label == 0 else 1 for label in dataset['test']['sentiment']]) # positive or negative sentiments"],"metadata":{"id":"G7d9FhJn-fOt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Model architecture\n","model = tf.keras.Sequential([\n","    tf.keras.layers.Embedding(10000, 16, input_length=120),\n","    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n","    tf.keras.layers.Dense(24, activation='relu'),\n","    tf.keras.layers.Dense(1, activation='sigmoid')\n","])\n","\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"],"metadata":{"id":"V2_XD6bi7mLh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Train model\n","model.fit(x_seq, y, epochs=10)"],"metadata":{"id":"pgkrZCPg9Z7Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#predict texts\n","y_pred = model.predict(x_seq)"],"metadata":{"id":"OdvqZkh674d1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["explainer = LimeTextExplainer(class_names=['Negative', 'Positive'])\n","\n","# Example text\n","idx = 0 # Index of the sample\n","text_to_explain = dataset['test']['text'][idx]\n","\n","#LIME requires a function that takes raw text as input and outputs prediction probabilities.\n","def predict_text(texts):\n","    # Tokenize and pad the input text\n","    sequences = get_sequences(tokenizer, texts)\n","    predictions = model.predict(sequences)\n","    # Convert predictions to two columns, one for each of the classes\n","    return np.hstack((1 - predictions, predictions))\n","\n","\n","# Generate explanation, num_features determines the number of features (words) to include in the explanation.\n","explanation = explainer.explain_instance(text_to_explain, predict_text, num_features=5)"],"metadata":{"id":"LiqjcwVz790x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#You can use the built in visualiser to show the interpretation\n","explanation.show_in_notebook(text=True)"],"metadata":{"id":"bFfbWcAsDIjs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Show the explanation\n","print(explanation.as_list())"],"metadata":{"id":"-sJ_rd3NAlio"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#alternatively you can visualise with pyplot\n","print(text_to_explain)\n","explanation.as_pyplot_figure();"],"metadata":{"id":"YeI95AEKCqfo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# LIME on images\n","\n","Again we will use the LIME package but this time apply it to a pretrained CNN model mobilenetV2."],"metadata":{"id":"OUbqwNSjJyE2"}},{"cell_type":"code","source":["# load pretrained model\n","model = tf.keras.applications.mobilenet_v2.MobileNetV2()"],"metadata":{"id":"kWPvyNL3mx_6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load the dataset it was trained on\n","ds , info = tfds.load('imagenet_v2', split='test', as_supervised=True, with_info=True)\n","\n","# Fetch the list of ImageNet classes\n","url = 'https://raw.githubusercontent.com/anishathalye/imagenet-simple-labels/master/imagenet-simple-labels.json'\n","imagenet_classes = requests.get(url).json()"],"metadata":{"id":"43LAg3qRK9J0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bs = 50 #batch size\n","\n","def preprocess(img, label): #normalise and reshape images for input to model\n","  img = img/255\n","  img = tf.image.resize(img, [224, 224])\n","  return img, label\n","\n","x_train = ds.map(preprocess).batch(bs).take(1) #get dataset"],"metadata":{"id":"uOe7E7rjOxNj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_pred = model.predict(x_train) #make predictions"],"metadata":{"id":"L6tf5s4_M4uE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_pred_idx = y_pred.argmax(axis=1) #indices of highest score prediction"],"metadata":{"id":"uLEkhsKdj5Tc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#process datasets into numpy arrays so easier to work with\n","npds =  tuple(zip(*x_train))\n","npx = np.asarray(npds[0])[0]\n","npy = np.asarray(npds[1]).reshape(-1)"],"metadata":{"id":"gNA-xu40R1pZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["explainer =  #** EDIT ME ** #use image explainer on images"],"metadata":{"id":"pu7dF05nehmb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Get lIME explaination of image\n","def get_lime(x, y, y_pred, labels, model, nlabels=2, nsamples=100):\n","  #x: input image\n","  #y: index of true label\n","  #y_pred: all predictions\n","  #labels: all labels\n","  #model: trained model\n","  #nlabels: top-n labels and explainatiions to plot\n","  #nsamples: number of perturbation samples, default=1000, higher values more computationally expensive\n","\n","  fig, ax = plt.subplots(1, (nlabels+1), figsize=((nlabels+1)*5, 5))\n","\n","  #get explainations of top prediction labels of instance\n","  explanation = explainer.explain_instance(x, model.predict, top_labels=nlabels, hide_color=0, num_samples=nsamples)\n","\n","  #show ground truth\n","  ax[0].imshow(x);\n","  ax[0].set_title(labels[y])\n","\n","  for i in range(nlabels):\n","    #mask important regions for each label\n","    #num_features=5 specifies the number of features (or in the case of images, superpixels) to include in the explanation.\n","    temp, mask = explanation.get_image_and_mask(explanation.top_labels[i], positive_only=False, num_features=5, hide_rest=False)\n","\n","    ax[i+1].imshow(mark_boundaries(temp / 2 + 0.5, mask))\n","    ax[i+1].set_title('p({}) = {:.4f}'.format(labels[explanation.top_labels[i]], y_pred[explanation.top_labels[i]]))\n","\n","  plt.show()\n","\n","idx = 0 #look at first prediction\n","get_lime(npx[idx], npy[idx], y_pred[0], imagenet_classes, model)"],"metadata":{"id":"WI6wFhZduhsa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# LIME from scratch\n","It's recommended that you use the official LIME package because its more robust and flexible and has loads of built in visualisation tools, but its worth taking a look how to implement LIME from scratch too."],"metadata":{"id":"YrQnmldUL-sf"}},{"cell_type":"code","source":["# First we create perturbed versions of the image by randomly masking out regions\n","def perturb_image(img, segments, num_perturb=500):\n","    perturbed_images = []\n","    masks = []\n","    for _ in range(num_perturb):\n","        mask = np.zeros(segments.shape)\n","        # Select a random subset of segments to turn off\n","        off_segments = np.random.choice(np.unique(segments), replace=False, size=np.random.randint(1, len(np.unique(segments))//2))\n","        for segment in off_segments:\n","            mask[segments == segment] = 1\n","        perturbed_img = img * np.expand_dims(mask, axis=-1)\n","        perturbed_images.append(perturbed_img)\n","        masks.append(mask)\n","    return np.array(perturbed_images), np.array(masks)\n","\n","\n","idx=0 #image to interpret\n","\n","# Segment image using the SLIC package with 50 segments, compactness 20 and sigma 1\n","segments_slic =  #** EDIT ME **\n","\n","# Generate perturbed images by randomly masking out segments\n","perturbed_images, masks =  #** EDIT ME **"],"metadata":{"id":"icrLStgyK69g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#plot example of perturbed image\n","fig, ax = plt.subplots(1,2)\n","ax[0].imshow(perturbed_images[0])\n","ax[1].imshow(masks[0]);"],"metadata":{"id":"3xVfvJZ1Pcdw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["predictions =  #** EDIT ME ** #apply model to perturbed images"],"metadata":{"id":"5azmguPxLtGF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Use the top prediction from the original image as the target class\n","top_prediction =  #** EDIT ME **\n","\n","# Get the prediction probabilities for the top class of each of the perturbed iamges\n","target_class_probs =  #** EDIT ME **"],"metadata":{"id":"-S3QSElCMqWR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we train an interpretable machine learning model like linear regression or decision tree on the perturbed images' predictions, using the masks as features to approximate the local decision boundary around the original prediction."],"metadata":{"id":"c9ki2G56M8xH"}},{"cell_type":"code","source":["# Train a Ridge model (linear regression with regularization, linear too simple)\n","linear_model = Ridge(alpha=1)\n","linear_model.fit(masks.reshape(masks.shape[0], -1), target_class_probs)\n","\n","# The coefficients indicate the importance of each segment\n","segment_importances = linear_model.coef_.reshape(segments_slic.shape)"],"metadata":{"id":"JZmvtG8bLzcX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig, ax = plt.subplots(1,2)\n","ax[0].imshow(npx[idx])\n","ax[0].set_title('input image')\n","ax[0].axis('off');\n","\n","ax[1].imshow(npx[idx])\n","ax[1].imshow(segment_importances, cmap='inferno', alpha=0.3)\n","ax[1].set_title('LIME explaination')\n","ax[1].axis('off');"],"metadata":{"id":"ep2QmBA8NQZJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# SHAPLEY"],"metadata":{"id":"Cf8NZ2fw4zIR"}},{"cell_type":"markdown","source":["In the SHAP package, some explainers are not compatible with TF2 so let's use their primary explainer. This is compatible with all models"],"metadata":{"id":"Dufu6lOZMYTq"}},{"cell_type":"code","source":["# create background data\n","masker = shap.maskers.Image(\"blur(128,128)\", npx[idx].shape)"],"metadata":{"id":"COvDJ3aRf9EO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create explainer\n","explainer = shap.Explainer(model, masker, output_names=imagenet_classes, algorithm='partition')"],"metadata":{"id":"MnrJgtDxiwSE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Explain the image with 100 evaluation shap values, more evaluations gives more granualarity but slower to run. Would recommend 300-500. Here we show top 8 examples\n","shap_values = explainer(np.expand_dims(npx[idx], axis=0), max_evals=100, batch_size=10, outputs=shap.Explanation.argsort.flip[:5])"],"metadata":{"id":"OqTqjcKEjNWa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Using the image_plot function in the package shap, plot the shap values\n","\n"," #** EDIT ME **"],"metadata":{"id":"jgQDJ8xokWRp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Here superpixels with positive SHAP values (red) contribute positively towards the prediction. Blue contribute negatively. We used the partition explainer which doesnt assume features (superpixels) are independent from each other. SHAP from scratch is similar to LIME with the main difference being that we substitute a set of features with their average across the sample and some spatial weighting so we will not do it here, but its worth familiarising with."],"metadata":{"id":"FaBYEL6gqlUC"}},{"cell_type":"markdown","source":["# SHAP for income prediction"],"metadata":{"id":"TBmyJRCJ4cLz"}},{"cell_type":"code","source":["# Load the Adult consenus dataset from Huggingface\n","dataset = load_dataset(\"scikit-learn/adult-census-income\")\n","\n","# Accessing the dataset\n","df = pd.DataFrame(dataset['train'])\n","\n","# Remove rows with missing data ('?' entries)\n","df.replace('?', pd.NA, inplace=True)  # Replace '?' with NA\n","df.dropna(inplace=True)  # Drop rows with NA"],"metadata":{"id":"I4D2yI232hAG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Separate features and target\n","X = df.drop(columns=['income'])\n","y = df['income']\n","\n","# Apply one-hot encoding to categorical columns\n","X= pd.get_dummies(X, drop_first=True)\n","y = pd.get_dummies(y, drop_first=True)\n","\n","# Normalize features\n","numerical_cols = X.select_dtypes(include=['float64', 'int64']).columns\n","scaler = StandardScaler()\n","X[numerical_cols] = scaler.fit_transform(X[numerical_cols])\n","\n","# Split into training and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"],"metadata":{"id":"HjOGwPew4oWM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define the model\n","model = tf.keras.Sequential([\n","    tkl.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n","    tkl.Dense(32, activation='relu'),\n","    tkl.Dense(1, activation='sigmoid')\n","])\n","\n","# Compile the model\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","# Train the model\n","history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.1)"],"metadata":{"id":"JAsktIXa2hbO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def f(X):\n","    return model.predict(X).flatten()"],"metadata":{"id":"nwukn-eG94cd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["explainer = shap.KernelExplainer(f, X_train.iloc[:50, :]) #use first 50 samples as baseline/background\n","shap_values = explainer.shap_values(X_train.iloc[299, :], nsamples=500) #to explain sample 299"],"metadata":{"id":"hcBK-Q4P96yZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["np.set_printoptions(precision=2)\n","\n","shap.force_plot(explainer.expected_value, shap_values, X_train.columns, matplotlib=matplotlib)"],"metadata":{"id":"2040Cbyy-Hye"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["shap_values50 = explainer.shap_values(X_train.iloc[280:330, :], nsamples=500) #explain many samples"],"metadata":{"id":"TDWERL_722Y0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["shap.initjs()\n","shap.force_plot(explainer.expected_value, shap_values50, X_train.iloc[280:330, :])"],"metadata":{"id":"U9PXwcon4_NZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["shap.decision_plot(explainer.expected_value, shap_values50, X_train.columns)"],"metadata":{"id":"1gYeRU9-E5xn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["decision plots are clearly useful to show outliers"],"metadata":{"id":"iMvwiCBCFy5x"}},{"cell_type":"markdown","source":["# Feature visualisation\n","\n","Now let's focus on methods more specific to neural nets and in particular CNNs"],"metadata":{"id":"pL0qEXGYW-vb"}},{"cell_type":"code","source":["# Load the pre-trained MobileNetv2 model but without the top layer to focus on convolutional layers\n","base_model =  #** EDIT ME **"],"metadata":{"id":"fPUpO1pAL4Ja"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["base_model.summary() #print model layers"],"metadata":{"id":"Lb_hSABbRwoa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create a new model that outputs the activations of a specific layer\n","layer_name = 'block_10_expand'  # Choose the layer\n","layer_output = base_model.get_layer(layer_name).output\n","model =  #** EDIT ME **"],"metadata":{"id":"v9f1f56hRTIC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["idx = 3 # choose an image to explain\n","\n","#get activation map of chosen layer\n","activation_map =  #** EDIT ME **"],"metadata":{"id":"KnsTLlJQcOz0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#plot first 4 filters and the input image\n","\n","fig, ax = plt.subplots(1,5, figsize=(20,5),gridspec_kw = {'wspace':0.05, 'hspace':0})\n","for i in range(4):\n","  ax[i].imshow(activation_map[0,:,:,i])\n","  ax[i].set_axis_off()\n","\n","ax[4].imshow(npx[idx])\n","ax[4].set_axis_off();"],"metadata":{"id":"ItZE6FyraTez"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Deprocess the image and make sure its within [0,255] pixel values\n","def deprocess_image(x):\n","    x *= 255\n","    x = np.clip(x, 0, 255).astype('uint8')\n","    return x\n","\n","# Function to generate patterns for filters\n","def generate_pattern(filter_index, size=224, steps=50, step_size=0.1):\n","    input_img_data = tf.random.uniform((1, size, size, 3)) #start with randomly initisalised image\n","\n","    # Run gradient ascent to maximise loss\n","    for _ in range(steps):\n","        with tf.GradientTape() as tape:\n","            tape.watch(input_img_data)\n","            activation = model(input_img_data)\n","            loss = tf.reduce_mean(activation[..., filter_index])\n","\n","        grads = tape.gradient(loss, input_img_data)\n","        normalized_grads = grads / (tf.sqrt(tf.reduce_mean(tf.square(grads))) + 1e-5) #normalise gradient by rms and small epsilon value for stability\n","        input_img_data += normalized_grads * step_size\n","\n","    img = input_img_data[0].numpy()\n","    return deprocess_image(img)"],"metadata":{"id":"zdlF4YD8L7FS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Filter index to visualize\n","filter_index = 2  # Can be any valid index of a filter in the chosen layer\n","\n","# Generate the pattern for a specific filter\n","plt.imshow(generate_pattern(filter_index))\n","plt.axis('off');"],"metadata":{"id":"aNUMgrUeFGaI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Saliency\n","feature visualisation is great, and looks amazing but practically its not feasible to visually inspect each filter, and its not always obvious what its showing anyway. Instead let's try to interpret directly on the image, which parts are important in a model's decision making process"],"metadata":{"id":"GTh5AuhHCe5p"}},{"cell_type":"code","source":["# we first need to load the full pretrained mobilenet again, (including top layer)\n","model =  #** EDIT ME **"],"metadata":{"id":"87SN-97HXErB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["idx = 32 #choose image\n","image = tf.expand_dims(npx[idx],axis=0) #reshape input image\n","\n","# Choose the index of the output class you want to visualize\n","class_idx = 36\n","\n","# Define the loss function to get the output of the specified class\n","def loss(output):\n","    return output[0][class_idx]\n","\n","# Get the gradients of the loss w.r.t the input image\n","with tf.GradientTape() as tape:\n","    tape.watch(image)\n","    prediction = model(image)\n","    loss_value = loss(prediction)\n","\n","grads =  #** EDIT ME **\n","\n","# Normalize the gradients\n","grads = tf.dtypes.cast(grads, tf.float32)\n","normalized_grads =  #** EDIT ME **\n","\n","# Take absolute gradients as saliency map\n","saliency_map =  #** EDIT ME **"],"metadata":{"id":"yqozG4OVjtpG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#for visualisation, we aggregate across channels, here we take the maximum, but you could do mean\n","saliency_map_gray =  #** EDIT ME **\n","\n","# Normalize the saliency map for better visualization to values [0,1]\n","norm_saliency_map =  #** EDIT ME **"],"metadata":{"id":"q2fqIcYU51rj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Make Saliency plots\n","fig,ax = plt.subplots(1,3,figsize=(20, 15),gridspec_kw = {'wspace':0.02, 'hspace':0})\n","\n","# Display original image\n","ax[0].set_title('Original Image')\n","ax[0].imshow(image[0])\n","ax[0].axis('off')\n","\n","# Display heatmap overlay on the original image\n","ax[1].set_title('Saliency Map Overlay')\n","ax[1].imshow(image[0])  # Show the original image\n","ax[1].imshow(norm_saliency_map, cmap='inferno', alpha=0.7)  # Overlay the saliency map with transparency\n","ax[1].axis('off')\n","\n","ax[2].set_title('Saliency Map')\n","ax[2].imshow(norm_saliency_map, cmap='inferno')  # Overlay the saliency map with transparency\n","ax[2].axis('off');"],"metadata":{"id":"CMDo2XDbzq4d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# GradCAM\n","The results of the saliency map can be difficult to interpret as they are too fine. Instead backprop to the last layer only to get a more class specific interpretation."],"metadata":{"id":"Q-revrz6Cgv4"}},{"cell_type":"code","source":["# Create a new model that outputs the activations of a specific layer\n","layer_name =  #** EDIT ME **  # Choose the layer (usually the last layer)\n","\n","#first model outputs layer, second outputs predictions\n","CAM_model = tf.keras.models.Model([model.inputs], [model.get_layer(layer_name).output, model.output])"],"metadata":{"id":"fhBOq6SjHV91"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Compute gradients wrt output of layer\n","with tf.GradientTape() as tape:\n","  conv_outputs, predictions = CAM_model(image)\n","  loss = predictions[:, class_idx]\n","\n","grads =  #** EDIT ME **\n","\n","# Pool the gradients across the channels\n","pooled_grads =  #** EDIT ME **\n","\n","# Weight the outputs with the computed gradients and sum\n","conv_outputs = conv_outputs[0]\n","weighted_conv_outputs = tf.reduce_sum(tf.multiply(pooled_grads, conv_outputs), axis=-1)\n","\n","# Apply ReLU to the weighted feature map\n","heatmap =  #** EDIT ME **\n","heatmap = heatmap.numpy()"],"metadata":{"id":"QEsjHzM4CiE8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Visualise heatmap\n","fig,ax = plt.subplots(1,3,figsize=(20, 15),gridspec_kw = {'wspace':0.02, 'hspace':0})\n","\n","# Display original image\n"," #** EDIT ME **\n","\n","# Display heatmap\n"," #** EDIT ME **\n","\n","\n","# Display heatmap overlay on the original image\n"," #** EDIT ME **"],"metadata":{"id":"D7w9LTI4VusL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Guided Grad CAM\n","\n","In contrary to the saliency map, GradCAM is perhaps too coarse. Let's combine the two."],"metadata":{"id":"tw82OIFstDka"}},{"cell_type":"code","source":["#multiply the heatmap with the saliency map\n","guided_gradCAM =  #** EDIT ME **"],"metadata":{"id":"tLEfwxArU831"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Visualise heatmap\n","fig,ax = plt.subplots(1,3,figsize=(20, 15),gridspec_kw = {'wspace':0.02, 'hspace':0})\n","\n","# Display original image\n"," #** EDIT ME **\n","\n","# Display heatmap\n"," #** EDIT ME **\n","\n","\n","# Display heatmap overlay on the original image\n"," #** EDIT ME **"],"metadata":{"id":"oRS7L_16m-Sw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"eMCk7FADssfx"},"execution_count":null,"outputs":[]}]}