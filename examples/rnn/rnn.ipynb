{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rnn.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMX+g9nqtvRllzNMRE1mHnl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adammoss/MLiS2/blob/master/examples/rnn/rnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeUfbxv9ghmQ",
        "colab_type": "text"
      },
      "source": [
        "As an example application, let's build a language model that predicts the probability of a sentence of $\\tau$ words,\n",
        "\n",
        "$\n",
        "P \\left( \\boldsymbol{x}^{(1)}, \\boldsymbol{x}^{(2)}, \\ldots, \\boldsymbol{x}^{(\\tau)}    \\right) = \\prod_{t  = 1}^{\\tau} P \\left( \\boldsymbol{x}^{(i)} |  \\boldsymbol{x}^{(1)}, \\ldots, \\boldsymbol{x}^{(i-1)}    \\right) \n",
        "$\n",
        "\n",
        "where $\\boldsymbol{x}^{(t)}$ is a vector representing a word. For now, we will use one-hot encoding where the length of the word vector is equal to the vocabulary size. This means that all words are orthogonal to one another. \n",
        "\n",
        "We train the model by setting the targets to be equal to the inputs, shifted by one time-step, i.e. $\\boldsymbol{y}^{(t)} = \\boldsymbol{x}^{(t-1)}$. Once the model is trained, we can use the network to output the probability of the next word, given an input sequence. The model is  generative as it can be used to produce new outputs, conditional on its previous inputs. \n",
        "\n",
        "For this example we extract sentences from Internet Movie Database (IMDB) reviews. We use the Natural Language Toolkit (NLTK) to tokenise sentences into words. For each sentence, we add an additional token to the beginning and end of sentences to indicate where they start and stop.  To reduce the  training time, we restrict the vocabulary to the 1000 most common words, replacing all others by an unknown token. We also truncate sentences to a maximum of 10 words, and use a limited training set of 1000 sentences. Even so, training is slow, as we forward and back-propagate single training examples at a time. \n",
        "\n",
        "The sentence tokenisation is based on that in https://github.com/dennybritz/rnn-tutorial-rnnlm. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Alq1W_bCWoTM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "import itertools\n",
        "import operator\n",
        "import numpy as np\n",
        "import nltk\n",
        "import sys\n",
        "from datetime import datetime\n",
        "import os\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1Byt39K3Dua",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhl7G_SQ7wlH",
        "colab_type": "text"
      },
      "source": [
        "Download NLTK data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRyc8euxWpzL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "nltk.download(\"book\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBvLWBQt8EZ6",
        "colab_type": "text"
      },
      "source": [
        "Upload imdb_sentences.txt file (or another file containing a list of sentences if you wish)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfpGBFRj-DzH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not os.path.isfile('imdb_sentences.txt'):\n",
        "  from google.colab import files\n",
        "  uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHNYmTqB93G9",
        "colab_type": "text"
      },
      "source": [
        "Add sentence start and end tags, convert to lower case and strip newlines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wyBM3rWv9chG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentence_start_token = \"SENTENCE_START\"\n",
        "sentence_end_token = \"SENTENCE_END\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8QVY1IkAYEA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('imdb_sentences.txt', 'r') as f:\n",
        "  sentences = f.readlines()\n",
        "sentences = [\"%s %s %s\" % (sentence_start_token, x.lstrip().rstrip('.\\n').lower(), sentence_end_token) for x in sentences]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPIWW5lqtSLG",
        "colab_type": "code",
        "outputId": "526654d7-5d1d-4833-9428-15e691ba929a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "print(\"Parsed %d sentences.\" % (len(sentences)))\n",
        "for i in range(0, 10):\n",
        "  print(\"Example: %s\" % sentences[i])"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parsed 12188 sentences.\n",
            "Example: SENTENCE_START story of a man who has unnatural feelings for a pig SENTENCE_END\n",
            "Example: SENTENCE_START starts out with a opening scene that is a terrific example of absurd comedy SENTENCE_END\n",
            "Example: SENTENCE_START a formal orchestra audience is turned into an insane, violent mob by the crazy chantings of it's singers SENTENCE_END\n",
            "Example: SENTENCE_START unfortunately it stays absurd the whole time with no general narrative eventually making it just too off putting SENTENCE_END\n",
            "Example: SENTENCE_START even those from the era should be turned off SENTENCE_END\n",
            "Example: SENTENCE_START the cryptic dialogue would make shakespeare seem easy to a third grader SENTENCE_END\n",
            "Example: SENTENCE_START on a technical level it's better than you might think with some good cinematography by future great vilmos zsigmond SENTENCE_END\n",
            "Example: SENTENCE_START future stars sally kirkland and frederic forrest can be seen briefly SENTENCE_END\n",
            "Example: SENTENCE_START airport '77 starts as a brand new luxury 747 plane is loaded up with valuable paintings & such belonging to rich businessman philip stevens (james stewart) who is flying them & a bunch of vip's to his estate in preparation of it being opened to the public as a museum, also on board is stevens daughter julie (kathleen quinlan) & her son SENTENCE_END\n",
            "Example: SENTENCE_START the luxury jetliner takes off as planned but mid-air the plane is hi-jacked by the co-pilot chambers (robert foxworth) & his two accomplice's banker (monte markham) & wilson (michael pataki) who knock the passengers & crew out with sleeping gas, they plan to steal the valuable cargo & land on a disused plane strip on an isolated island but while making his descent chambers almost hits an oil rig in the ocean & loses control of the plane sending it crashing into the sea where it sinks to the bottom right bang in the middle of the bermuda triangle SENTENCE_END\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTBDO_fs-udT",
        "colab_type": "text"
      },
      "source": [
        "Tokenize the sentences into words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRfoIQZV-tNL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDJ1NGJy-7i_",
        "colab_type": "code",
        "outputId": "27c7fb55-47e6-499a-9e80-11980ab56dc6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "word_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
        "print(\"Found %d unique words tokens.\" % len(word_freq.items()))"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 18154 unique words tokens.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZGpHm2s_IyE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size = 1000\n",
        "unknown_token = 'UNKNOWN_TOKEN'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBTJmo8G_Fv0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = word_freq.most_common(vocab_size-1)\n",
        "index_to_word = [x[0] for x in vocab]\n",
        "index_to_word.append(unknown_token)\n",
        "word_to_index = dict([(w,i) for i, w in enumerate(index_to_word)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-C-wm5h_kQB",
        "colab_type": "text"
      },
      "source": [
        "Replace all words not in our vocabulary with the unknown token and discard sentences under min / over max number of words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZS0xWviykoTl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "min_sentence_length = 5\n",
        "truncate_sentence_length = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXT2EKyw_h6p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "purged_sentences = []\n",
        "for i, sent in enumerate(tokenized_sentences):\n",
        "  if len(sent) >= min_sentence_length:\n",
        "    purged_sentences.append([w if w in word_to_index else unknown_token for w in sent[0:truncate_sentence_length]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r09ZMRxqk7pC",
        "colab_type": "code",
        "outputId": "3d41f3d2-19bf-4840-8be9-ab20671b64e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "print(\"Purged %d sentences.\" % (len(purged_sentences)))\n",
        "for i in range(0, 10):\n",
        "  print(\"Example: %s\" % purged_sentences[i])"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Purged 11872 sentences.\n",
            "Example: ['SENTENCE_START', 'story', 'of', 'a', 'man', 'who', 'has', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', 'for']\n",
            "Example: ['SENTENCE_START', 'starts', 'out', 'with', 'a', 'opening', 'scene', 'that', 'is', 'a']\n",
            "Example: ['SENTENCE_START', 'a', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', 'audience', 'is', 'turned', 'into', 'an', 'UNKNOWN_TOKEN']\n",
            "Example: ['SENTENCE_START', 'unfortunately', 'it', 'UNKNOWN_TOKEN', 'absurd', 'the', 'whole', 'time', 'with', 'no']\n",
            "Example: ['SENTENCE_START', 'even', 'those', 'from', 'the', 'UNKNOWN_TOKEN', 'should', 'be', 'turned', 'off']\n",
            "Example: ['SENTENCE_START', 'the', 'UNKNOWN_TOKEN', 'dialogue', 'would', 'make', 'UNKNOWN_TOKEN', 'seem', 'UNKNOWN_TOKEN', 'to']\n",
            "Example: ['SENTENCE_START', 'on', 'a', 'UNKNOWN_TOKEN', 'level', 'it', \"'s\", 'better', 'than', 'you']\n",
            "Example: ['SENTENCE_START', 'future', 'stars', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', 'and', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', 'can', 'be']\n",
            "Example: ['SENTENCE_START', 'airport', 'UNKNOWN_TOKEN', 'starts', 'as', 'a', 'UNKNOWN_TOKEN', 'new', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN']\n",
            "Example: ['SENTENCE_START', 'the', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', 'takes', 'off', 'as', 'UNKNOWN_TOKEN', 'but', 'UNKNOWN_TOKEN']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fx-hXzf4_8g1",
        "colab_type": "text"
      },
      "source": [
        "Create the training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnTX5tex_zg_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = np.asarray([[word_to_index[w] for w in sent[:-1]] for sent in purged_sentences])\n",
        "Y_train = np.asarray([[word_to_index[w] for w in sent[1:]] for sent in purged_sentences])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUwaGdwHIBtc",
        "colab_type": "code",
        "outputId": "9c98b834-4727-4270-ba8f-8c27da0d5e24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"Example: \", X_train[2])"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Example:  [1, 4, 999, 999, 278, 8, 465, 99, 43]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpzQxszlDOYz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax(x):\n",
        "    xt = np.exp(x - np.max(x))\n",
        "    return xt / np.sum(xt)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Yr2h159_7F2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNN:\n",
        "    \n",
        "  def __init__(self, word_dim, hidden_dim=100):\n",
        "      # Assign instance variables\n",
        "      self.word_dim = word_dim\n",
        "      self.hidden_dim = hidden_dim\n",
        "      # Randomly initialize the network parameters\n",
        "      self.U = np.random.uniform(-np.sqrt(1./word_dim), np.sqrt(1./word_dim), (word_dim, hidden_dim))\n",
        "      self.V = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (hidden_dim, word_dim))\n",
        "      self.W = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (hidden_dim, hidden_dim))\n",
        "      self.b = np.zeros(hidden_dim)\n",
        "      self.c = np.zeros(word_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Do a forward pass for single example\n",
        "    T = len(x)\n",
        "    h = np.zeros((T , self.hidden_dim))\n",
        "    o = np.zeros((T, self.word_dim))\n",
        "    for t in range(T):\n",
        "      # Note that we are indexing U by x[t]. This is the same as multiplying U with a one-hot vector.\n",
        "      h[t] = self.U[x[t], :] + self.b\n",
        "      if t > 1:\n",
        "        h[t] += np.matmul(self.W.T, h[t-1])\n",
        "      h[t] = np.tanh(h[t])\n",
        "      o[t] = softmax(np.matmul(self.V.T, h[t]) + self.c)\n",
        "    return (o, h)\n",
        "\n",
        "  def backward(self, x, y, clip_value=None):\n",
        "    #Do a backward pass for single example\n",
        "    T = len(x)\n",
        "    o, h = self.forward(x)\n",
        "    # Accumulate the gradients in these variables\n",
        "    dLdU = np.zeros(self.U.shape)\n",
        "    dLdV = np.zeros(self.V.shape)\n",
        "    dLdW = np.zeros(self.W.shape)\n",
        "    dLdb = np.zeros(self.b.shape)\n",
        "    dLdc = np.zeros(self.c.shape)\n",
        "    # dL/do\n",
        "    delta_o = o\n",
        "    delta_o[np.arange(len(y)), y] -= 1.\n",
        "    # dL/dh\n",
        "    delta_h = np.zeros((T, self.hidden_dim))\n",
        "    for t in reversed(range(T)):\n",
        "      delta_h[t] = np.matmul(self.V, delta_o[t, :])\n",
        "      if t < T - 1:\n",
        "        delta_h[t] += np.matmul(np.matmul(self.W, np.diag(1 - h[t+1]**2)), delta_h[t+1])\n",
        "    # Accumulate gradients over time-steps\n",
        "    for t in range(T):\n",
        "      dLdc += delta_o[t, :]\n",
        "      dLdb += (1 - h[t]**2) * delta_h[t, :]\n",
        "      dLdV += np.outer(h[t, :], delta_o[t, :])\n",
        "      if t > 0:\n",
        "        dLdW += np.matmul(np.outer(h[t-1, :], delta_h[t, :]), np.diag(1 - h[t]**2))\n",
        "      xm = np.zeros((self.word_dim))\n",
        "      xm[x] = 1.\n",
        "      dLdU += np.matmul(np.outer(xm, delta_h[t, :]), np.diag(1 - h[t]**2))\n",
        "    if clip_value is not None:\n",
        "      dLdb = np.clip(dLdb, -clip_value, clip_value)\n",
        "      dLdc = np.clip(dLdc, -clip_value, clip_value)\n",
        "      dLdV = np.clip(dLdV, -clip_value, clip_value)\n",
        "      dLdW = np.clip(dLdW, -clip_value, clip_value)\n",
        "      dLdU = np.clip(dLdU, -clip_value, clip_value)\n",
        "    return (dLdU, dLdV, dLdW, dLdb, dLdc)\n",
        "\n",
        "  def step(self, x, y, learning_rate=0.01):\n",
        "    #Â Perform SGD step for single example\n",
        "    dLdU, dLdV, dLdW, dLdb, dLdc  = self.backward(x, y)\n",
        "    self.U -= learning_rate * dLdU\n",
        "    self.V -= learning_rate * dLdV\n",
        "    self.W -= learning_rate * dLdW\n",
        "    self.b -= learning_rate * dLdb\n",
        "    self.c -= learning_rate * dLdc\n",
        "\n",
        "  def loss(self, x, y):\n",
        "    # Per example loss\n",
        "    o, h = self.forward(x)\n",
        "    return - np.sum(o[np.arange(len(y)), y])\n",
        "\n",
        "  def generate_sentence(self, max_length=20):\n",
        "    # We start the sentence with the start token\n",
        "    new_sentence = [word_to_index[sentence_start_token]]\n",
        "    # Repeat until we get an end token or reach maximum sentence length\n",
        "    while not new_sentence[-1] == word_to_index[sentence_end_token] and len(new_sentence) < max_length:\n",
        "      o, h = self.forward(new_sentence)\n",
        "      sampled_word = word_to_index[unknown_token]\n",
        "      # We don't want to sample unknown words or sentence start\n",
        "      while sampled_word == word_to_index[unknown_token] or sampled_word == word_to_index[sentence_start_token]:\n",
        "          samples = np.random.multinomial(1, o[-1])\n",
        "          sampled_word = np.argmax(samples)\n",
        "      new_sentence.append(sampled_word)\n",
        "    sentence_str = [index_to_word[x] for x in new_sentence]\n",
        "    return sentence_str\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRw6_OG4CNt3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = RNN(vocab_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbYNOpEQM7sI",
        "colab_type": "text"
      },
      "source": [
        "Generate random sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UI5anlSCSxe",
        "colab_type": "code",
        "outputId": "76d6ab80-08ad-49ab-f44f-25deb92d9e09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "for i in range(10):\n",
        "  print(model.generate_sentence())"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['SENTENCE_START', 'viewers', 'business', 'been', 'credits', 'may', 'which', 'happened', 'wanted', 'keep', 'days', 'cool', 'woody', 'spent', 'during', \"'re\", 'yourself', 'ends', 'anil', 'somehow']\n",
            "['SENTENCE_START', 'must', 'watched', 'people', 'monster', 'cut', 'simply', 'viewers', 'version', 'guys', 'managed', 'down', 'does', 'badly', 'far', 'lame', 'anything', 'horrible', 'during', 'making']\n",
            "['SENTENCE_START', 'blame', 'forget', 'appear', 'woody', 'huge', 'our', 'another', 'before', 'van', 'line', 'liked', 'van', 'three', 'job', 'minutes', 'may', 'off', 'much', 'total']\n",
            "['SENTENCE_START', 'entertaining', 'parts', 'among', 'pointless', 'most', 'romance', \"''\", 'points', 'll', 'change', 'cast', 'deserves', 'air', 'done', 'bring', 'write', 'you', 'performances', 'saying']\n",
            "['SENTENCE_START', 'freeman', 'stick', 'stories', 'my', 'ca', 'possibly', 'through', 'few', 'completely', 'easily', 'will', 'tone', 'sorry', 'your', 'moment', 'viewing', 'funny', 'gore', 'show']\n",
            "['SENTENCE_START', 'j', 'full', 'ask', 'way', 'between', 'hours', 'laughable', 'slow', 'thats', 'went', 'moment', 'avoid', 'work', 'favorite', 'production', 'zero', 'laughed', 'video', 'akshay']\n",
            "['SENTENCE_START', 'for', 'j', 'movie', 'cinema', 'biggest', 'freeman', 'happy', 'beautiful', 'was', 'yet', 'knows', 'takes', '``', 'bergman', 'them', 'ago', 'simply', 'bar', 'killed']\n",
            "['SENTENCE_START', 'business', 'doing', 'job', 'airport', 'half', 'happy', 'wo', 'full', 'piece', 'class', 'road', 'actress', 'romance', 'unbelievable', 'budget', 'level', 'sister', 'nothing', 'imdb']\n",
            "['SENTENCE_START', 'forced', 'tells', 'goes', 'leading', 'tell', 'hit', 'viewing', 'utterly', 'different', 'oh', 'thats', 'tries', 'sequences', 'name', 'jokes', 'went', 'kill', 'rest', 'a']\n",
            "['SENTENCE_START', 'problem', 'of', 'fine', 'slow', 'not', 'go', 'acted', 'unbelievable', 'their', 'copy', 'damme', 'against', 'act', 'thought', 'going', 'acharya', 'wish', 'dead', 'victor']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDAjHknvCb7b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_epochs = 100\n",
        "learning_rate = 0.01"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WG1piZGyUHw",
        "colab_type": "text"
      },
      "source": [
        "Limit training examples to save time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5umSeVdByQTg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train[0:1000]\n",
        "Y_train = Y_train[0:1000]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcVe2NKPZlk5",
        "colab_type": "code",
        "outputId": "46ab48ec-36fa-480c-d255-7b401d201bae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "loss_history = []\n",
        "for epoch in range(num_epochs):\n",
        "  loss = 0\n",
        "  for i in range(len(X_train)):\n",
        "    loss += model.loss(X_train[i], Y_train[i])\n",
        "  loss = loss / len(X_train)\n",
        "  print(\"Epoch {0} Loss {1}\".format(epoch , loss))\n",
        "  loss_history.append(loss)\n",
        "  for i in range(len(X_train)):\n",
        "    model.step(X_train[i], Y_train[i], learning_rate=learning_rate)\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 Loss -0.008668172033746894\n",
            "Epoch 1 Loss -0.3035566640598766\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lGGVmE3hkMg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(6, 6))\n",
        "ax = plt.subplot(1, 1, 1)\n",
        "ax.plot(loss_history[:])\n",
        "ax.set_xlabel('Epoch', fontsize=14)\n",
        "ax.set_ylabel('Loss', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmpzCd6-KMru",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(10):\n",
        "  print(model.generate_sentence())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D16DIBo1Kbep",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}